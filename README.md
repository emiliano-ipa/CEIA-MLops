#  CEIA - Operaciones de aprendizaje autom√°tico I | Proyecto Final

Repositorio para el proyecto final de la materia **Operaciones de aprendizaje autom√°tico I** de la Carrera de Especializaci√≥n en Inteligencia Artificial (CEIA - UBA).

---

## üë• Integrantes del Grupo

- **Martin Brocca** (<martinbrocca@gmail.com>)
- **Natalia Espector** (<nataliaespector@gmail.com>)
- **Emiliano Iparraguirre** (<emiliano.iparraguirre22@gmail.com>)
- **Agustin Lopez Fredes** (<agustin.lopezfredes@gmail.com>)
- **Fermin Rodriguez del Castillo** (<ferkrodriguez98@gmail.com>)

---

# Proyecto Final de MLOps1

Este proyecto tiene como objetivo desplegar un entorno **end-to-end** de MLOps que:
- Orquesta procesos de ETL y entrenamiento de modelos con **Apache Airflow**.
- Realiza **b√∫squeda de hiperpar√°metros** y tracking de experimentos con **MLflow**.
- Simula un entorno productivo con contenedores Docker para todos los servicios involucrados.
- Permite **servir modelos** a trav√©s de una API REST (FastAPI).
- Utiliza **MinIO** como Data Lake (simulaci√≥n de Amazon S3) para almacenamiento de datos y artefactos.

## üóÇÔ∏è Servicios incluidos

- [Apache Airflow](https://airflow.apache.org/) ‚Äì Orquestaci√≥n de pipelines y ejecuci√≥n de DAGs.
- [MLflow](https://mlflow.org/) ‚Äì Tracking de experimentos, registro de modelos y m√©tricas.
- [FastAPI](https://fastapi.tiangolo.com/) ‚Äì API REST para servir modelos.
- [MinIO](https://min.io/) ‚Äì Almacenamiento de datos tipo S3.
- [PostgreSQL](https://www.postgresql.org/) ‚Äì Base de datos relacional (para MLflow y Airflow).
- [ValKey](https://valkey.io/) ‚Äì Base de datos key-value utilizada por Airflow internamente.

![Diagrama de servicios](diagrama_MLOps1.png)

Por defecto, al iniciar el entorno se crean los siguientes **buckets**:

- `s3://data`
- `s3://mlflow` (usado por MLflow para guardar artefactos)

Y las siguientes **bases de datos**:

- `mlflow_db` (para MLflow)
- `airflow` (para Airflow)

---

## üéØ Objetivo del proyecto

Implementar modelos de aprendizaje de m√°quina en un entorno productivo simulado con **Docker Compose**. Los DAGs incluidos realizan:

- **Optimizaci√≥n de hiperpar√°metros** (con [Optuna](https://optuna.org/)) en 10 corridas r√°pidas para acortar los tiempos de ejecuci√≥n.
- Tracking de m√©tricas y par√°metros en **MLflow**.
- Registro de modelos en un **experimento com√∫n** que centraliza la comparaci√≥n de performance.

Cada DAG tiene como objetivo **entrenar y comparar modelos supervisados de clasificaci√≥n** con diferentes algoritmos (KNN, SVM y LightGBM).

---

## üìú DAGs implementados

### `etl_mlflow.py`
Este DAG realiza un **pipeline ETL** (Extract ‚Äì Transform ‚Äì Load) sobre los datos de empleados que luego usar√°n los modelos:
- **Extract:** descarga desde MinIO el dataset crudo (`enriched_employee_dataset.csv`).
- **Transform:** limpia columnas irrelevantes, elimina valores nulos, divide los datos en train/test y realiza imputaciones.
- **Load:** sube los datasets procesados (`X_train`, `X_test`, `y_train`, `y_test`) al bucket `processed` en MinIO.
- **Trackeo del ETL en MLflow**:
- Loguea par√°metros (por ejemplo, columnas eliminadas, split ratio).
- Loguea m√©tricas (cantidad de filas, missing values antes y despu√©s de limpiar).
- Loguea artefactos (datasets procesados) y estad√≠sticas descriptivas (media, desviaci√≥n est√°ndar de cada feature).
- Tambi√©n sube los archivos finales a MinIO para que otros DAGs los usen.

### `dag_knn.py`
DAG que entrena un modelo de clasificaci√≥n **K-Nearest Neighbors (KNN)**:
- Descarga los datos procesados desde MinIO.
- Ejecuta **10 trials con Optuna** para optimizar hiperpar√°metros (`n_neighbors`, `weights`, `algorithm`).
- Registra en MLflow los resultados de cada trial en el experimento `knn_optuna` (F1 y par√°metros).
- Entrena un modelo final con los mejores par√°metros.
- Eval√∫a el modelo y guarda en un **experimento com√∫n**:
  - F1, precision, recall.
  - Matriz de confusi√≥n (como imagen).
  - Par√°metros finales y el modelo entrenado.

### `dag_svm.py`
DAG que entrena un modelo de **Support Vector Machine (SVM)** con el mismo flujo que KNN:
- Descarga datos desde MinIO.
- Corre 10 trials de Optuna optimizando `C`, `kernel` y `gamma`.
- Guarda m√©tricas y par√°metros en `svm_optuna`.
- Entrena el mejor modelo y lo registra en el **experimento com√∫n** junto con todas las m√©tricas y artefactos.

### `dag_lightgbm.py`
DAG que entrena un modelo de **LightGBM**:
- Descarga datos procesados desde MinIO.
- Ejecuta 10 trials de Optuna optimizando par√°metros clave (`n_estimators`, `learning_rate`, `max_depth`, `num_leaves`, `subsample`, `colsample_bytree`).
- Registra los trials en `lightgbm_optuna` y entrena un modelo final con los mejores par√°metros.
- Loguea en MLflow m√©tricas, matriz de confusi√≥n y modelo final. A diferencia de los anteriores, usa `mlflow.lightgbm.log_model` para registrar el modelo final.

---

## ‚öôÔ∏è Instalaci√≥n y configuraci√≥n

### 1Ô∏è‚É£ Instalar Docker
Instal√° [Docker Desktop](https://docs.docker.com/engine/install/) en tu computadora.

### 2Ô∏è‚É£ Clonar el repositorio
```bash
git clone https://github.com/martinbrocca/CEIA-MLops.git
cd CEIA-MLops
```

### 3Ô∏è‚É£ Crear carpetas necesarias
```bash
mkdir -p airflow/config airflow/dags airflow/logs airflow/plugins
```

### 4Ô∏è‚É£ Ajustar UID de Airflow (solo en Linux/MacOS)
Edit√° `.env` y reemplaz√° `AIRFLOW_UID` por el de tu usuario:
```bash
id -u <username>
```

### 5Ô∏è‚É£ Levantar todos los servicios
```bash
docker compose --profile all up
```

üìç **Accesos:**
- Apache Airflow ‚Üí [http://localhost:8080](http://localhost:8080)
- MLflow ‚Üí [http://localhost:5001](http://localhost:5001)
- MinIO ‚Üí [http://localhost:9001](http://localhost:9001)
- API REST ‚Üí [http://localhost:8800/](http://localhost:8800/)
- Docs API (Swagger) ‚Üí [http://localhost:8800/docs](http://localhost:8800/docs)

‚ö†Ô∏è **LightGBM:**
Si vas a correr el DAG de LightGBM, ejecut√° un rebuild antes:
```bash
docker compose --profile all up --build
```

---

## ‚èπÔ∏è C√≥mo apagar los servicios

Cuando no uses los servicios, pod√©s apagarlos para liberar memoria:

```bash
docker compose --profile all down
```

Para **eliminar toda la infraestructura** (im√°genes, vol√∫menes y datos):

```bash
docker compose down --rmi all --volumes
```

üìå **Nota:** Esto borra todos los buckets y bases de datos.

---

## ‚ö†Ô∏è Consideraciones para entornos Windows

En Windows, para evitar conflictos al reconstruir la imagen de Airflow, se recomienda **crear la imagen solo una vez** y que todos los servicios la utilicen, en vez de que cada uno intente reconstruirla.

En Windows, Docker Desktop maneja las rutas de forma diferente y puede provocar que **cada servicio de Airflow intente reconstruir la imagen `extending_airflow` en paralelo**. Esto genera errores como:

- image already exists (la imagen ya existe)
- conflictos de acceso a archivos temporales
- builds interrumpidos y contenedores que no terminan de levantarse

Para evitar estos problemas, lo mejor es **crear la imagen una sola vez** y luego indicarle a todos los servicios de Airflow que usen esa imagen, en vez de dejar que cada contenedor intente reconstruirla.

üìå **1Ô∏è‚É£ Construir la imagen de Airflow manualmente**
Desde la ra√≠z del proyecto (CEIA-MLops), ejecut√°:

```powershell
docker build -t extending_airflow:latest ./dockerfiles/airflow
```

‚úÖ Esto dejar√° lista una imagen llamada `extending_airflow:latest`.

üìå **2Ô∏è‚É£ Editar docker-compose.yml**

Busc√° el bloque:

```yaml
x-airflow-common: &airflow-common
  build: './dockerfiles/airflow'
  image: ${AIRFLOW_IMAGE_NAME:-extending_airflow}
  profiles:
    - airflow
    - all
```

**Borr√° la l√≠nea de `build:`** y dejalo as√≠:

```yaml
x-airflow-common: &airflow-common
  image: extending_airflow:latest
  profiles:
    - airflow
    - all
```

üëâ Esto le indica a Docker Compose que ya existe la imagen y no tiene que construirla cada vez.

üìå **3Ô∏è‚É£ Levantar todos los servicios**

Ahora corr√©:

```powershell
docker compose --profile all up
```

---

## üîß Airflow ‚Äì detalles importantes

### Variables de entorno
Pod√©s modificar las variables de entorno de Airflow en `docker-compose.yml` dentro de `x-airflow-common`. M√°s info: [configuraci√≥n de Airflow](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html).

### Ejecutor Celery
Airflow usa [Celery Executor](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/executor/celery.html), lo que significa que las tareas se ejecutan en contenedores separados (workers).

### CLI de Airflow
Para usar la CLI:
```bash
docker compose --profile all --profile debug up
```
Luego, por ejemplo:
```bash
docker-compose run airflow-cli config list
```
M√°s info: [CLI de Airflow](https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html).

### Variables y conexiones
- Variables: `secrets/variables.yaml`
- Conexiones: `secrets/connections.yaml`

Las conexiones agregadas en la UI **no persisten** si borr√°s los contenedores. Las definidas en YAML s√≠ persisten, pero **no aparecen** en la UI.

---

## ‚òÅÔ∏è Conexi√≥n con MinIO

Variables de entorno:
```bash
AWS_ACCESS_KEY_ID=minio
AWS_SECRET_ACCESS_KEY=minio123
AWS_ENDPOINT_URL_S3=http://localhost:9000
```

MLflow necesita:
```bash
MLFLOW_S3_ENDPOINT_URL=http://localhost:9000
```

Esto permite usar `boto3`, `awswrangler`, etc. como si fuera un S3 real.

‚ö†Ô∏è Si trabaj√°s con AWS real, cuidado de no sobreescribir tus credenciales.

---

## üóÑÔ∏è Valkey

Valkey es usado por Airflow para manejar su backend. Actualmente no expone su puerto, pero puede modificarse el `docker-compose.yml` si quisieras usarlo externamente.

---

### ‚úÖ Estado actual
‚úîÔ∏è Infraestructura Docker lista.
‚úîÔ∏è DAGs de **KNN, SVM y LightGBM** para optimizaci√≥n y comparaci√≥n de modelos.
‚úîÔ∏è Tracking en MLflow.
‚úîÔ∏è API REST b√°sica en FastAPI.